Sex classification, for the purpose of this study, will consist of classification between male and female voices. The dataset that is being used for this project also comes from a Crowd-sourced Emotional Multimodal Actors Dataset (CREMA-D). 

The model code also comes from the following repository https://github.com/m3hrdadfi/soxan. The Wav2Vec2 model is used with a classification head. 

The dataset containing the metadata about each audio file looks like the table below: 

![image](https://user-images.githubusercontent.com/54903276/152843271-67f07983-4983-4eaa-ba55-9c9efb1fe255.png)

The class distribution follows the table below:

![image](https://user-images.githubusercontent.com/54903276/152843351-cd9b7a4e-714d-44b0-95d1-9516014f0636.png)

