Sex classification, for the purpose of this study, will consist of classification between male and female voices. The dataset that is being used for this project also comes from a Crowd-sourced Emotional Multimodal Actors Dataset (CREMA-D). 

The model code also comes from the following repository https://github.com/m3hrdadfi/soxan. The Wav2Vec2 model is used with a classification head. 

The dataset containing the metadata about each audio file looks like the table below: 

![image](https://user-images.githubusercontent.com/54903276/152843271-67f07983-4983-4eaa-ba55-9c9efb1fe255.png)

The class distribution follows the table below:

![image](https://user-images.githubusercontent.com/54903276/152843449-5da08a68-a4e3-45e9-a2f9-447c8c97d507.png)


